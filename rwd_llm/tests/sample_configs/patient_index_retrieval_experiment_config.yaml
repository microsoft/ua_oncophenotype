# Experiment that runs RAG (index-based retrieval) QA on a sample of the MIMIC dataset
defaults:
  - config/experiment/dataset: mimic_small_dataset
  - config/experiment/chain: index_chain

# some options for the experiment

# model_name: "gpt-4"
# deployment_name: ${model_name}
model_name: gpt-3.5-turbo
deployment_name: gpt-35-turbo

azure_openai:
  class:
    _target_: "hydra.utils.get_class"
    path: "langchain.chat_models.AzureChatOpenAI"
  extra_args:
    deployment_name: ${deployment_name}
    model_name: ${model_name}

openai:
  class:
    _target_: "hydra.utils.get_class"
    path: "langchain.chat_models.ChatOpenAI"
  extra_args:
    model_name: ${model_name}

# choose the llm to use
llm: ${openai}

# the actual config
config:
  _target_: rwd_llm.experiment_config.ExperimentConfig
  experiment:
    _target_: rwd_llm.experiment.Experiment
    chain:
      # override some values from the chain specified in defults
      llm_class: ${llm.class}
      llm_extra_args: ${llm.extra_args}
    data_runner:
      _target_: rwd_llm.experiment.data_runners.DatasetRunner
      n_threads: 1
    evaluation:
      _target_: rwd_llm.eval.ClassificationEvaluation
  output_dir: "experiment_output"
